{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## END TO END DATA PIPELINE PROJECT\n",
    "SCRAPPING DATA(HEADLINES, CORRESPONDING HYPERLINKS and STORY) FROM AN ONLINE NEWS WEBSITE (THEGUARDIAN.COM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries Successfully Imported!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup as BS\n",
    "print(\"Libraries Successfully Imported!\")\n",
    "#In this cell, I basically imported the necessary package required to scrap the site i.e BEAUTIFULSOUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.theguardian.com/au\"\n",
    "extracting_data = requests.get(url).text\n",
    "wiki_data = BS(extracting_data, 'html.parser')\n",
    "\n",
    "# Here, I scrapped ALL of the contents on the page of the link as html tags using the \n",
    "# html.parser and the beautifulsoup package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "Headlines = []      # I will be scrapping the news headline, story link and contents; 3 parameters\n",
    "links = []\n",
    "story =[]\n",
    "storys = []\n",
    "data = []       # Creating empty lists to hold our data\n",
    "i=0             # I created this int to serve as an iterator to iterate through the 'links' list and get the article contents for each link by incrementing it in every for loop\n",
    "\n",
    "for news in wiki_data.findAll('div' , {'class':'fc-item__container'}): # This is the tag where the headline and link is contained\n",
    "    datum = {} # An empty dict to hold the the scapped data\n",
    "    Headlines.append(news.text.replace('\\n',''))\n",
    "    links.append(news.a['href'])\n",
    "    page = requests.get(links[i])\n",
    "    Object = BS(page.content) # This is the tag to get the story content for each of the links\n",
    "    for storyss in Object.findAll('div', {'class':'article-body-commercial-selector'}):\n",
    "        story.append(storyss.text.replace('\\n',''))\n",
    "    \n",
    "    if len(story) != i+1:\n",
    "        story.append('No Story Available')\n",
    "\n",
    "    datum['Headlines'] = Headlines[i]\n",
    "    datum['Links'] = links[i]\n",
    "    datum ['Story'] = story[i]\n",
    "    \n",
    "    data.append(datum) #The final list holding all of our data\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headlines</th>\n",
       "      <th>Links</th>\n",
       "      <th>Story</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>Explain it to me quickly    Why is TikTok obse...</td>\n",
       "      <td>https://www.theguardian.com/culture/2023/jan/0...</td>\n",
       "      <td>OK Calla, apparently there is a new TikTok tre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>Edward Norton  Actor is direct descendant of P...</td>\n",
       "      <td>https://www.theguardian.com/film/2023/jan/05/e...</td>\n",
       "      <td>Genealogical records reviewed on the show Find...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>I left my baby to write this. How do artis...</td>\n",
       "      <td>https://www.theguardian.com/commentisfree/2023...</td>\n",
       "      <td>Since having a baby, I have never felt more cr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>US Congress  Fears over lax security in Republ...</td>\n",
       "      <td>https://www.theguardian.com/us-news/2023/jan/0...</td>\n",
       "      <td>Two years after the January 6 insurrection, fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>Top shots  Epiphany celebrations and RuPaul’s ...</td>\n",
       "      <td>https://www.theguardian.com/uk-news/gallery/20...</td>\n",
       "      <td>No Story Available</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Headlines  \\\n",
       "89  Explain it to me quickly    Why is TikTok obse...   \n",
       "90  Edward Norton  Actor is direct descendant of P...   \n",
       "91      I left my baby to write this. How do artis...   \n",
       "92  US Congress  Fears over lax security in Republ...   \n",
       "93  Top shots  Epiphany celebrations and RuPaul’s ...   \n",
       "\n",
       "                                                Links  \\\n",
       "89  https://www.theguardian.com/culture/2023/jan/0...   \n",
       "90  https://www.theguardian.com/film/2023/jan/05/e...   \n",
       "91  https://www.theguardian.com/commentisfree/2023...   \n",
       "92  https://www.theguardian.com/us-news/2023/jan/0...   \n",
       "93  https://www.theguardian.com/uk-news/gallery/20...   \n",
       "\n",
       "                                                Story  \n",
       "89  OK Calla, apparently there is a new TikTok tre...  \n",
       "90  Genealogical records reviewed on the show Find...  \n",
       "91  Since having a baby, I have never felt more cr...  \n",
       "92  Two years after the January 6 insurrection, fr...  \n",
       "93                                 No Story Available  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data) #checking the length of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.theguardian.com/fashion/2023/jan/07/happy-to-let-it-hang-out-budgie-smugglers-are-back-on-australian-beaches'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[2]['Links'] # Visualizing the eleventh element for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertManyResult at 0x2389b61e770>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# load_dotenv(find_dotenv())\n",
    "# password = os.environ.get(\"MONGODB\")\n",
    "\n",
    "conn_str = f\"mongodb+srv://pelvic:Pelvic4ever@cluster0.zst9xg2.mongodb.net/?retryWrites=true&w=majority\"\n",
    "client = MongoClient(conn_str)  # This is the link required to access my mongodb hosted on atlas\n",
    "db = client.Online_News.news # I then created a new collection in my 'Online_News' database\n",
    "db.insert_many(data) # I finally inserted all of my data into the database\n",
    "# dbs = client.list_database_names()\n",
    "# print(dbs)\n",
    "# testdb = client.Online_News\n",
    "# collections = testdb.list_collection_names()\n",
    "# print(collections)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('Kubeflow_Pipelines-master-oUkMXjJq')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0ad50f5c4eb016e033faa80e5e094e3843f36ce778b98bf7ef2521b490b1589d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
